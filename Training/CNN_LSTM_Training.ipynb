{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-LSTM Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hohJWoVHrVYk"
      },
      "source": [
        "!pip install --upgrade pip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBUevliJrftW"
      },
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UjRY8fjrl7v"
      },
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "import tensorflow as tf\n",
        "from random import shuffle\n",
        "from tqdm import tqdm_notebook\n",
        "import itertools\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3FWUjQIrsJi",
        "outputId": "a34c606e-0dd5-4c96-bcb6-b2457284fc15"
      },
      "source": [
        "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu,True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "print(tf.__version__)\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "\n",
        "#policy = mixed_precision.Policy('mixed_float16')\n",
        "\n",
        "mixed_precision.set_policy(policy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
            "  Tesla P100-PCIE-16GB, compute capability 6.0\n",
            "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydVGRKD9rzB6"
      },
      "source": [
        "dataset = 'Custom/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zX5zyIar7uy"
      },
      "source": [
        "num_images = 10\n",
        "img_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Cg5kJMsDpC"
      },
      "source": [
        "def Xception2(input_shape=(None,None,None,3), lr = 1e-3):\n",
        "    channel_axis = 3\n",
        "    img_input = layers.Input(input_shape)\n",
        "    x = layers.TimeDistributed(layers.Conv2D(32, (3, 3), strides=(2, 2), use_bias=False, name='block1_conv1'))(img_input)\n",
        "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block1_conv1_bn'))(x)\n",
        "    x = layers.TimeDistributed(layers.Activation('relu', name='block1_conv1_act'))(x)\n",
        "    x = layers.TimeDistributed(layers.Conv2D(64, (3, 3), use_bias=False, name='block1_conv2'))(x)\n",
        "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block1_conv2_bn'))(x)\n",
        "    x = layers.TimeDistributed(layers.Activation('relu', name='block1_conv2_act'))(x)\n",
        "\n",
        "    residual = layers.TimeDistributed(layers.Conv2D(128, (1, 1), strides=(2, 2), padding='same', use_bias=False))(x)\n",
        "    residual = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis))(residual)\n",
        "\n",
        "    x = layers.TimeDistributed(layers.SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1'))(x)\n",
        "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block2_sepconv1_bn'))(x)\n",
        "    x = layers.TimeDistributed(layers.Activation('relu', name='block2_sepconv2_act'))(x)\n",
        "    x = layers.TimeDistributed(layers.SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2'))(x)\n",
        "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block2_sepconv2_bn'))(x)\n",
        "\n",
        "    x = layers.TimeDistributed(layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block2_pool'))(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    residual = layers.TimeDistributed(layers.Conv2D(256, (1, 1), strides=(2, 2), padding='same', use_bias=False))(x)\n",
        "    residual = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis))(residual)\n",
        "\n",
        "    x = layers.TimeDistributed(layers.Activation('relu', name='block3_sepconv1_act'))(x)\n",
        "    x = layers.TimeDistributed(layers.SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1'))(x)\n",
        "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block3_sepconv1_bn'))(x)\n",
        "    x = layers.TimeDistributed(layers.Activation('relu', name='block3_sepconv2_act'))(x)\n",
        "    x = layers.TimeDistributed(layers.SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2'))(x)\n",
        "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block3_sepconv2_bn'))(x)\n",
        "\n",
        "    x = layers.TimeDistributed(layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block3_pool'))(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    residual = layers.TimeDistributed(layers.Conv2D(728, (1, 1), strides=(2, 2), padding='same', use_bias=False))(x)\n",
        "    residual = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis))(residual)\n",
        "\n",
        "    x = layers.TimeDistributed(layers.Activation('relu', name='block4_sepconv1_act'))(x)\n",
        "    x = layers.TimeDistributed(layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1'))(x)\n",
        "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block4_sepconv1_bn'))(x)\n",
        "    x = layers.TimeDistributed(layers.Activation('relu', name='block4_sepconv2_act'))(x)\n",
        "    x = layers.TimeDistributed(layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2'))(x)\n",
        "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block4_sepconv2_bn'))(x)\n",
        "\n",
        "    x = layers.TimeDistributed(layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block4_pool'))(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    for i in range(8):\n",
        "        residual = x\n",
        "        prefix = 'block' + str(i + 5)\n",
        "\n",
        "        x = layers.TimeDistributed(layers.Activation('relu', name=prefix + '_sepconv1_act'))(x)\n",
        "        x = layers.TimeDistributed(layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1'))(x)\n",
        "        x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name=prefix + '_sepconv1_bn'))(x)\n",
        "        x = layers.TimeDistributed(layers.Activation('relu', name=prefix + '_sepconv2_act'))(x)\n",
        "        x = layers.TimeDistributed(layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2'))(x)\n",
        "        x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name=prefix + '_sepconv2_bn'))(x)\n",
        "        x = layers.TimeDistributed(layers.Activation('relu', name=prefix + '_sepconv3_act'))(x)\n",
        "        x = layers.TimeDistributed(layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3'))(x)\n",
        "        x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name=prefix + '_sepconv3_bn'))(x)\n",
        "\n",
        "        x = layers.add([x, residual])\n",
        "\n",
        "    residual = layers.TimeDistributed(layers.Conv2D(1024, (1, 1), strides=(2, 2), padding='same', use_bias=False))(x)\n",
        "    residual = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis))(residual)\n",
        "\n",
        "    x = layers.TimeDistributed(layers.Activation('relu', name='block13_sepconv1_act'))(x)\n",
        "    x = layers.TimeDistributed(layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1'))(x)\n",
        "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block13_sepconv1_bn'))(x)\n",
        "    x = layers.TimeDistributed(layers.Activation('relu', name='block13_sepconv2_act'))(x)\n",
        "    x = layers.TimeDistributed(layers.SeparableConv2D(1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2'))(x)\n",
        "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block13_sepconv2_bn'))(x)\n",
        "\n",
        "    x = layers.TimeDistributed(layers.MaxPooling2D((3, 3),\n",
        "                            strides=(2, 2),\n",
        "                            padding='same',\n",
        "                            name='block13_pool'))(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    x = layers.TimeDistributed(layers.SeparableConv2D(1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1'))(x)\n",
        "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block14_sepconv1_bn'))(x)\n",
        "    x = layers.TimeDistributed(layers.Activation('relu', name='block14_sepconv1_act'))(x)\n",
        "\n",
        "    x = layers.TimeDistributed(layers.SeparableConv2D(2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2'))(x)\n",
        "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block14_sepconv2_bn'))(x)\n",
        "    x = layers.TimeDistributed(layers.Activation('relu', name='block14_sepconv2_act'))(x)\n",
        "\n",
        "    x = layers.TimeDistributed(layers.GlobalAveragePooling2D(name='avg_pool'))(x)\n",
        "    \n",
        "    model = models.Model(img_input, x, name='xception')\n",
        "    model.compile(optimizer = Adam(lr), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_callbacks(modelname, es_patience, rlr_patience, verbose = 1):\n",
        "    checkpointer = ModelCheckpoint(modelname+'.h5', monitor = 'val_loss', save_best_only = True, verbose = verbose, mode = 'min')\n",
        "    #earlystopper = EarlyStopping(monitor = 'val_loss', patience = es_patience, verbose = verbose, mode = 'min')\n",
        "    reduceLR = ReduceLROnPlateau(monitor = 'val_loss', factor = 1/np.sqrt(10), patience = rlr_patience, cooldown = 1 ,verbose = verbose, mode = 'min')\n",
        "    return checkpointer,reduceLR\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtRZGsmJsKOB"
      },
      "source": [
        "def generator(l, batch_size):\n",
        "    gen = iter(itertools.cycle(l))\n",
        "    while 1:\n",
        "        yield [next(gen) for _ in range(batch_size)]\n",
        "\n",
        "def get_input(dataset,split,label, folder):\n",
        "    images = np.zeros((num_images,img_size,img_size,3), dtype = np.float32)\n",
        "    imagespath = [f for f in os.listdir(dataset+split+label+folder+\"/\") if 'jpg' in f]\n",
        "    #print(dataset+label+folder)\n",
        "    for i in range(num_images):\n",
        "        img = cv2.resize(plt.imread(dataset+split+label+folder+\"/\"+imagespath[i]), (img_size, img_size), interpolation=cv2.INTER_CUBIC)/255\n",
        "        images[i] = img\n",
        "    return [images]\n",
        "\n",
        "def video_data_generator(dataset, mode = 'Training/', batch_size = 8):\n",
        "    \n",
        "    if mode == 'Testing/':\n",
        "        batch_size = batch_size//4 \n",
        "\n",
        "    assert batch_size % 2 == 0\n",
        "    \n",
        "    real_folders = [f for f in os.listdir(dataset+mode+\"Real/\") if '.ipynb' not in f]\n",
        "    fake_folders = [f for f in os.listdir(dataset+mode+\"Fake/\") if '.ipynb' not in f]\n",
        "    \n",
        "    #shuffle(real_folders)\n",
        "    #shuffle(fake_folders)\n",
        "    \n",
        "   # real_gen = generator(real_folders, batch_size = batch_size // 1)\n",
        "   # fake_gen = generator(fake_folders, batch_size = batch_size // 1)\n",
        "    real_gen = generator(real_folders, batch_size = batch_size // 2)#not changed becoz testing time jhol chances\n",
        "    fake_gen = generator(fake_folders, batch_size = batch_size // 2)\n",
        "    \n",
        "    while True:\n",
        "\n",
        "        real_folder_batch, fake_folder_batch = next(real_gen), next(fake_gen)\n",
        "        batch_input = []\n",
        "        batch_output = []\n",
        "        for real_folder, fake_folder in zip(real_folder_batch, fake_folder_batch):\n",
        "            \n",
        "            batch_input += get_input(dataset,mode, 'Real/', real_folder)\n",
        "            batch_input += get_input(dataset,mode, 'Fake/', fake_folder)\n",
        "            batch_output += [0., 1.]\n",
        "        # Return a tuple of (input,output) to feed the network\n",
        "        batch_x = np.array(batch_input)\n",
        "        batch_y = np.array(batch_output)\n",
        "\n",
        "        yield (batch_x, batch_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntczkmcZyB7R"
      },
      "source": [
        "#confusion matrix function\n",
        "def make_confusion_matrix(cf,\n",
        "                          group_names=None,\n",
        "                          categories='auto',\n",
        "                          count=True,\n",
        "                          percent=True,\n",
        "                          cbar=True,\n",
        "                          xyticks=True,\n",
        "                          xyplotlabels=True,\n",
        "                          sum_stats=True,\n",
        "                          figsize=None,\n",
        "                          cmap='Blues',\n",
        "                          title=None):\n",
        "    '''\n",
        "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
        "    Arguments\n",
        "    ---------\n",
        "    cf:            confusion matrix to be passed in\n",
        "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
        "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
        "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
        "    normalize:     If True, show the proportions for each category. Default is True.\n",
        "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
        "                   Default is True.\n",
        "    xyticks:       If True, show x and y ticks. Default is True.\n",
        "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
        "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
        "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
        "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
        "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "                   \n",
        "    title:         Title for the heatmap. Default is None.\n",
        "    '''\n",
        "\n",
        "\n",
        "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
        "    blanks = ['' for i in range(cf.size)]\n",
        "\n",
        "    if group_names and len(group_names)==cf.size:\n",
        "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
        "    else:\n",
        "        group_labels = blanks\n",
        "\n",
        "    if count:\n",
        "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
        "    else:\n",
        "        group_counts = blanks\n",
        "\n",
        "    if percent:\n",
        "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
        "    else:\n",
        "        group_percentages = blanks\n",
        "\n",
        "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
        "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
        "\n",
        "\n",
        "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
        "    if sum_stats:\n",
        "        #Accuracy is sum of diagonal divided by total observations\n",
        "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
        "\n",
        "        #if it is a binary confusion matrix, show some more stats\n",
        "        if len(cf)==2:\n",
        "            #Metrics for Binary Confusion Matrices\n",
        "            precision = cf[1,1] / sum(cf[:,1])\n",
        "            recall    = cf[1,1] / sum(cf[1,:])\n",
        "            f1_score  = 2*precision*recall / (precision + recall)\n",
        "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
        "                accuracy,precision,recall,f1_score)\n",
        "        else:\n",
        "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
        "    else:\n",
        "        stats_text = \"\"\n",
        "\n",
        "\n",
        "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
        "    if figsize==None:\n",
        "        #Get default figure size if not set\n",
        "        figsize = plt.rcParams.get('figure.figsize')\n",
        "\n",
        "    if xyticks==False:\n",
        "        #Do not show categories if xyticks is False\n",
        "        categories=False\n",
        "\n",
        "\n",
        "    # MAKE THE HEATMAP VISUALIZATION\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
        "\n",
        "    if xyplotlabels:\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label' + stats_text)\n",
        "    else:\n",
        "        plt.xlabel(stats_text)\n",
        "    \n",
        "    if title:\n",
        "        plt.title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JXK49uNy4pe"
      },
      "source": [
        "#accuracy and losses function for plotting\n",
        "\n",
        "def summarize_diagnostics(history,epoch):\n",
        "    # plot loss\n",
        "    plt.subplot(121)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(history.history['loss'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_loss'], color='orange', label='val')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    # plot accuracy\n",
        "    plt.subplot(122)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], color='orange', label='val')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.subplots_adjust(top=0.92, bottom=0.2, left=0.0, right=2.5, hspace=0.25,\n",
        "                    wspace=0.25)\n",
        "    plt.show()    \n",
        "\n",
        "\n",
        "    #     pyplot.subplot(213)\n",
        "    # \tpyplot.title('Classification Accuracy')\n",
        "    # \tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    # \tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "    # save plot to file\n",
        "    #filename = sys.argv[0].split('/')[-1]\n",
        "    #pyplot.savefig(filename +'InceptionNewAdam'+str(epoch)+ '_plot.png')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijhFw8gIsO-8"
      },
      "source": [
        "# Video\n",
        "#from tensorflow.keras.applications import Xception\n",
        "model_path = \"/content/drive/My Drive/All_Models/\" # Save models here\n",
        "Path(model_path).mkdir(parents=True, exist_ok = True)\n",
        "\n",
        "epochs = 20\n",
        "batch_size =64\n",
        "\n",
        "train_spe = len(os.listdir(dataset+\"Training/Real\"))//batch_size\n",
        "print(train_spe)\n",
        "val_spe = len(os.listdir(dataset+\"Validation/Fake\"))//batch_size\n",
        "print(val_spe)\n",
        "test_spe = len(os.listdir(dataset+\"Testing/Fake\"))//(batch_size//4)\n",
        "print(test_spe)\n",
        "\n",
        "# Train multiple times\n",
        "attempt=0\n",
        "print('Attempt #',attempt)\n",
        "foldername = model_path+\"Custom_Dataset\"+\"_xceptionimagenet_attempt\"+str(attempt+1)+\"epoch_20/\"\n",
        "Path(foldername).mkdir(parents=True, exist_ok=True)\n",
        "modelname = foldername+\"_xceptionimagenet_\"+str(num_images)+\"_\"+str(img_size)+\"_attempt\"+str(attempt+1)\n",
        "train_datagen = video_data_generator(dataset,'Training/', batch_size =batch_size)\n",
        "val_datagen = video_data_generator(dataset,'Validation/', batch_size = batch_size)\n",
        "test_datagen = video_data_generator(dataset,'Testing/', batch_size = batch_size)\n",
        "base_model = Xception2() \n",
        "base_model.load_weights('imagenet_1.h5')\n",
        "base_model.trainable = False\n",
        "    \n",
        "inputs = layers.Input(shape=(None,None, None,3))\n",
        "outputs = base_model(inputs, training=False)\n",
        "outputs = layers.Dropout(0.4, name = 'lstm_dropout')(outputs)\n",
        "outputs = layers.LSTM(1, dtype='float32', name='LSTM_False')(outputs)\n",
        "outputs = (outputs * 0.5) + 0.5\n",
        "cnnlstm_model = models.Model(inputs, outputs, name='xception_cnnlstm')\n",
        "cnnlstm_model.compile(optimizer = Adam(1e-3), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "callbacks = get_callbacks(modelname, es_patience = 6, rlr_patience = 4, verbose = 1)\n",
        "history = cnnlstm_model.fit(train_datagen, verbose = 2, steps_per_epoch =train_spe, epochs = 20, validation_data = val_datagen, validation_steps = val_spe, callbacks = callbacks)\n",
        "best_valloss = callbacks[0].best\n",
        "cnnlstm_model = load_model(modelname+\".h5\")\n",
        "test_datagen = video_data_generator(dataset,'Testing/', batch_size = batch_size)\n",
        "print(cnnlstm_model.evaluate(test_datagen,steps=2*test_spe,verbose=0))\n",
        "\n",
        "print('I am best validation loss:')\n",
        "print(best_valloss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b7qLCehwzn0"
      },
      "source": [
        "#predictions\n",
        "#50bestvalloss 0.6355091333389282\n",
        "\n",
        "testy=np.zeros(shape=(256,1))\n",
        "for i in range (256):\n",
        "  if(i%2!=0):\n",
        "    testy[i]=float(1)\n",
        "  else:\n",
        "    testy[i]=float(0)\n",
        "\n",
        "yhat_classes=cnnlstm_model.predict(test_datagen,steps=2*test_spe,verbose=0)\n",
        "\n",
        "#ROC CURVE\n",
        "fpr, tpr, thresholds = roc_curve(testy, yhat_classes)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "print('Area under the Receiver Operating Characteristic curve:', roc_auc_score(testy, yhat_classes))\n",
        "\n",
        "#Confusion MAtrix\n",
        "cf_matrix=confusion_matrix(testy,yhat_classes.round())\n",
        "\n",
        "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
        "categories = ['Zero', 'One']\n",
        "make_confusion_matrix(cf_matrix, \n",
        "                      group_names=labels,\n",
        "                      categories=categories, \n",
        "                      cmap='Blues')\n",
        "plt.show()\n",
        "                      \n",
        "#Traning Validation accuracy as well as losses\n",
        "summarize_diagnostics(history,20)\n",
        "\n",
        "#PR curve\n",
        "precision, recall, thresholds = precision_recall_curve(testy, yhat_classes)\n",
        "plt.plot(recall, precision)\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqZS8ykXws2p"
      },
      "source": [
        "\n",
        "'''\n",
        "# Video\n",
        "#from tensorflow.keras.applications import Xception\n",
        "model_path = \"/content/drive/My Drive/All_Models/\" # Save models here\n",
        "Path(model_path).mkdir(parents=True, exist_ok = True)\n",
        "\n",
        "epochs = 30\n",
        "batch_size =32\n",
        "attempt=0\n",
        "print('Attempt #',attempt)\n",
        "foldername = model_path+\"Custom_Dataset\"+\"_xceptionimagenet_attempt\"+str(attempt+1)+\"epoch_30/\"\n",
        "Path(foldername).mkdir(parents=True, exist_ok=True)\n",
        "modelname = foldername+\"_xceptionimagenet_\"+str(num_images)+\"_\"+str(img_size)+\"_attempt\"+str(attempt+1)\n",
        "train_datagen = video_data_generator(dataset,'Training/', batch_size =batch_size)\n",
        "val_datagen = video_data_generator(dataset,'Validation/', batch_size = batch_size)\n",
        "test_datagen = video_data_generator(dataset,'Testing/', batch_size = batch_size)\n",
        "cnnlstm_model = load_model(\"/content/drive/MyDrive/All_Models/Custom_Dataset_xceptionimagenet_attempt1epoch_30/_xceptionimagenet_10_128_attempt1.h5\")\n",
        "train_spe = len(os.listdir(dataset+\"Training/Real\"))//batch_size\n",
        "print(train_spe)\n",
        "val_spe = len(os.listdir(dataset+\"Validation/Fake\"))//batch_size\n",
        "print(val_spe)\n",
        "test_spe = len(os.listdir(dataset+\"Testing/Fake\"))//(batch_size//4)\n",
        "print(test_spe)\n",
        "base_model = Xception2() \n",
        "base_model.load_weights('dataset/imagenet_1.h5')\n",
        "\n",
        "best_valloss=0.5887700915336609'''\n",
        "base_model.trainable = True\n",
        "cnnlstm_model.compile(optimizer = Adam(1e-6), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "callbacks = get_callbacks(modelname + '_finetuned', es_patience = 7, rlr_patience = 4, verbose = 1)\n",
        "callbacks[0].best = best_valloss\n",
        "history = cnnlstm_model.fit(train_datagen, verbose = 2, steps_per_epoch = train_spe, epochs = 20, validation_data = val_datagen, validation_steps = val_spe, callbacks = callbacks)\n",
        "cnnlstm_model = load_model(modelname+\".h5\")\n",
        "test_datagen = video_data_generator(dataset,'Testing/', batch_size = batch_size)\n",
        "print(cnnlstm_model.evaluate(test_datagen,steps=2*test_spe,verbose=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BQBhStzzTc0"
      },
      "source": [
        "#predictions\n",
        "testy=np.zeros(shape=(256,1))\n",
        "for i in range (256):\n",
        "  if(i%2!=0):\n",
        "    testy[i]=float(1)\n",
        "  else:\n",
        "    testy[i]=float(0)\n",
        "\n",
        "yhat_classes=cnnlstm_model.predict(test_datagen,steps=2*test_spe,verbose=0)\n",
        "\n",
        "#ROC CURVE\n",
        "fpr, tpr, thresholds = roc_curve(testy, yhat_classes)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "print('Area under the Receiver Operating Characteristic curve:', roc_auc_score(testy, yhat_classes))\n",
        "\n",
        "#Confusion MAtrix\n",
        "cf_matrix=confusion_matrix(testy,yhat_classes.round())\n",
        "\n",
        "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
        "categories = ['Zero', 'One']\n",
        "make_confusion_matrix(cf_matrix, \n",
        "                      group_names=labels,\n",
        "                      categories=categories, \n",
        "                      cmap='Blues')\n",
        "plt.show()\n",
        "                      \n",
        "#Traning Validation accuracy as well as losses\n",
        "summarize_diagnostics(history,20)\n",
        "\n",
        "#PR curve\n",
        "precision, recall, thresholds = precision_recall_curve(testy, yhat_classes)\n",
        "plt.plot(recall, precision)\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
