{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Video Deepfake Detection Using CNN, LSTM, GRU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4HuL3B1UNwS"
      },
      "source": [
        "## Installations and Import Statements \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbRGB3drUNw2",
        "outputId": "de6091e7-83d6-44fb-ea15-a8217cd7b8d8"
      },
      "source": [
        "!pip install --upgrade pip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\u001b[0m\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.5/dist-packages (20.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0ULyxPmUNw-",
        "outputId": "b56fbf4a-22d7-45aa-f2ca-78ec348b0c08"
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Python 3.5 reached the end of its life on September 13th, 2020. Please upgrade your Python as Python 3.5 is no longer maintained. pip 21.0 will drop support for Python 3.5 in January 2021. pip 21.0 will remove support for this functionality.\u001b[0m\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.5/dist-packages (2.3.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.34.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (0.33.4)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (0.33.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (0.33.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.5/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.5/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.15.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.34.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.5/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (41.0.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.5/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.5/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.5/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.5/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.25.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.5/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.5/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (41.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.5/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.5/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.5/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (41.0.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.5/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.26.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.5/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.25.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.5/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.5/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlxQa8YjUNxA"
      },
      "source": [
        "!pip install dlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKZZUZFnUNxB"
      },
      "source": [
        "import cv2\n",
        "import os \n",
        "import dlib\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import os\n",
        "from random import shuffle\n",
        "from tqdm import tqdm_notebook\n",
        "import itertools\n",
        "import shutil\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')#made change\n",
        "mixed_precision.set_policy(policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHmij1eYUNxE"
      },
      "source": [
        "# Functions for Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZf36JS5UNxH"
      },
      "source": [
        "def get_input(folder):\n",
        "    images = np.zeros((num_images,128,128,3), dtype = np.float32)\n",
        "    imagespath = range(0,2054)\n",
        "    for i in range(num_images):\n",
        "        a= str(imagespath[i]).zfill(2)\n",
        "        img = cv2.resize(plt.imread(folder+a+'.jpg'), (128, 128), interpolation=cv2.INTER_CUBIC)/255\n",
        "        #print(a)\n",
        "        images[i] = img\n",
        "        \n",
        "    return [images]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXnUC5foUNxJ"
      },
      "source": [
        "## CNN: Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n4eWuOHUNxK"
      },
      "source": [
        "model_path = 'models/CNN50_Tom_Cruise/'\n",
        "attempt = 0\n",
        "foldername = model_path+\"_xceptionimagenet_attempt\"+str(attempt)+\"/\"\n",
        "num_images = 2054\n",
        "img_size = 128\n",
        "modelname = foldername+\"_xceptionimagenet_\"+str(num_images)+\"_\"+str(img_size)\n",
        "cnn_model = load_model(modelname+\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL9zooaEUNxL"
      },
      "source": [
        "test_folder=\"../Tom_Cruise_Dataset/Tom_Cruise_dataset_two_faces/testing/real/TOM CRUISE 2020 Deepfake  VFX Breakdown  Side by Side_1080p_Trim/\"\n",
        "batch_input = get_input(test_folder)\n",
        "batch_x = np.array(batch_input)\n",
        "probr1 = cnn_model.predict(batch_x)\n",
        "test_folder=\"../Tom_Cruise_Dataset/Tom_Cruise_dataset_two_faces/testing/fake/TOM CRUISE 2020 Deepfake  VFX Breakdown  Side by Side_1080p_Trim/\"\n",
        "batch_input = get_input(test_folder)\n",
        "batch_x = np.array(batch_input)\n",
        "probf1 = cnn_model.predict(batch_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiFjzHgHUNxN"
      },
      "source": [
        "# CNN:Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW7nxvM_UNxO"
      },
      "source": [
        "p = \"../mmod_human_face_detector.dat\"\n",
        "cnn_face_detector = dlib.cnn_face_detection_model_v1(p)\n",
        "for i in range(1):\n",
        "    try:\n",
        "        vidcap = cv2.VideoCapture('../Tom_Cruise/TOM CRUISE 2020 Deepfake VFX Breakdown Side by Side_1080p_Trim.mp4')\n",
        "        success,image = vidcap.read()\n",
        "        count = 0\n",
        "        success = True\n",
        "        frames_path=\"../Tom_Cruise/Completeframe/TOM_CRUISE_2020_Deepfake_VFX_Breakdown_Side_by_Side_1080p_Trim/\"\n",
        "        print(\"NOW FRAMING\")\n",
        "        imagespath = range(0,2241)\n",
        "        kr=0\n",
        "        kf=0\n",
        "        prediction_folder_path='Tom_Cruise/Reconstructed/frames1/'\n",
        "        Path(prediction_folder_path).mkdir(parents=True, exist_ok = True)\n",
        "        real_count_real=0\n",
        "        fake_count_real=0\n",
        "        real_count_fake=0\n",
        "        fake_count_fake=0\n",
        "        print(\"NOW DETECTING\")\n",
        "        m=0\n",
        "        #Perform face detection\n",
        "        for it in range(0,2241):\n",
        "            #try:\n",
        "            a= str(imagespath[it]).zfill(4)\n",
        "            image=cv2.imread(frames_path+a+'.jpg')\n",
        "            \n",
        "            if image is None:\n",
        "                print(\"Could not read input image\")\n",
        "                continue\n",
        "\n",
        "            faces_cnn = cnn_face_detector(image[:,:,::-1],0)\n",
        "            \n",
        "            if(len(faces_cnn)!=2):\n",
        "                print(\"Total Faces =     \" ,len(faces_cnn))\n",
        "                cv2.imwrite(prediction_folder_path+a+'.jpg',image)\n",
        "                continue\n",
        "            else:\n",
        "                odd=1\n",
        "                for face in faces_cnn:\n",
        "                    \n",
        "                    x = face.rect.left()\n",
        "                    y = face.rect.top()\n",
        "                    w= face.rect.right() - x\n",
        "                    h= face.rect.bottom() - y \n",
        "                    y1 =y-50\n",
        "                    y2= y+h+30\n",
        "                    x1=x-15\n",
        "                    x2=x+w+50\n",
        "                    \n",
        "                    if odd==1:\n",
        "                        odd=0\n",
        "                        result=probr1[0,kr,0]\n",
        "                        kr+=1\n",
        "                        if(kr==2054):\n",
        "                            print(a)\n",
        "                        if(result<0.5):\n",
        "                            print(\"real\")\n",
        "                            cv2.rectangle(image, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "                            cv2.putText(image, 'REAL:'+str(result), (30, 250), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (35,102,11), thickness=2,lineType=cv2.LINE_AA) \n",
        "                            real_count_real+=1\n",
        "                            \n",
        "                        else:\n",
        "                            print(\"fake\")\n",
        "                            cv2.rectangle(image, (x1,y1), (x2,y2), (0,0,255), 2)\n",
        "                            cv2.putText(image, 'FAKE:'+str(result), (30, 250), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0,0,255), thickness=2,lineType=cv2.LINE_AA) \n",
        "                            fake_count_real+=1\n",
        "                        real_count= real_count_real\n",
        "                        fake_count= fake_count_real\n",
        "                        cv2.putText(image, 'REAL:'+str(real_count), (30, 400), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (35,102,11),thickness=2, lineType=cv2.LINE_AA) \n",
        "                        cv2.putText(image, 'FAKE:'+str(fake_count), (30, 450), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0,0,255), thickness=2,lineType=cv2.LINE_AA) \n",
        "                        cv2.imwrite(prediction_folder_path+a+'.jpg',image)\n",
        "                        \n",
        "                    else:\n",
        "                        odd=1\n",
        "                        result=probf1[0,kf,0]\n",
        "                        kf+=1\n",
        "                        if(kf==2054):\n",
        "                            print(a)\n",
        "                        if(result<0.5):\n",
        "                            print(\"real\")\n",
        "                            cv2.rectangle(image, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "                            cv2.putText(image, 'REAL:'+str(result), (1600, 250), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (35,102,11), thickness=2,lineType=cv2.LINE_AA) \n",
        "                            real_count_fake+=1\n",
        "                            \n",
        "                        else:\n",
        "                            print(\"fake\")\n",
        "                            cv2.rectangle(image, (x1,y1), (x2,y2), (0,0,255), 2)\n",
        "                            cv2.putText(image, 'FAKE:'+str(result), (1600, 250), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0,0,255), thickness=2,lineType=cv2.LINE_AA) \n",
        "                            fake_count_fake+=1\n",
        "                        real_count= real_count_fake\n",
        "                        fake_count= fake_count_fake\n",
        "                        cv2.putText(image, 'REAL:'+str(real_count), (1600, 400), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (35,102,11),thickness=2, lineType=cv2.LINE_AA) \n",
        "                        cv2.putText(image, 'FAKE:'+str(fake_count), (1600, 450), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0,0,255), thickness=2,lineType=cv2.LINE_AA) \n",
        "                        cv2.imwrite(prediction_folder_path+a+'.jpg',image)\n",
        "                print(it)\n",
        "        img_array = []\n",
        "        for j in range(num_images):\n",
        "            \n",
        "            a= str(imagespath[j]).zfill(4)\n",
        "            \n",
        "            try:    \n",
        "                img = cv2.imread(prediction_folder_path+a+'.jpg')\n",
        "                height, width, layers = img.shape\n",
        "                size = (width,height)\n",
        "                img_array.append(img)\n",
        "            except:\n",
        "                continue\n",
        "        print(\"NOW RECONSTRUCTING\")\n",
        "        save_path='Tom_Cruise/Reconstructed/rvideo/'\n",
        "        Path(save_path).mkdir(parents=True, exist_ok = True)\n",
        "        out = cv2.VideoWriter(save_path+'Tom_Cruise_video'+'.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 30, size)\n",
        "\n",
        "        for i in range(len(img_array)):\n",
        "            out.write(img_array[i])\n",
        "        out.release()\n",
        "        print(\"VIDEO COMPLETED:\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoLVcJK9UNxc"
      },
      "source": [
        "## Video Deepfake Detection Using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MuEsA-kUNxd"
      },
      "source": [
        "Required Function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRONFsQGUNxf"
      },
      "source": [
        "num_images = 20\n",
        "img_size = 128\n",
        "\n",
        "def get_input( folder):\n",
        "    images = np.zeros((num_images,img_size,img_size,3), dtype = np.float32)\n",
        "    imagespath = [f for f in os.listdir(folder+\"/\") if 'jpg' in f]\n",
        "    #print(dataset+label+folder)\n",
        "    for i in range(num_images):\n",
        "        img = cv2.resize(plt.imread(folder+\"/\"+imagespath[i]), (img_size, img_size), interpolation=cv2.INTER_CUBIC)/255\n",
        "        images[i] = img\n",
        "    return [images]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBWrS4IHUNxg"
      },
      "source": [
        "CNN+ LSTM Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkVx3vIhUNxh"
      },
      "source": [
        "test_folder1=\"../Tom_Cruise_Dataset/Tom_Cruise_dataset_two_faces/testing/fake/TOM CRUISE 2020 Deepfake  VFX Breakdown  Side by Side_1080p_Trim\"\n",
        "test_folder2=\"../Tom_Cruise_Dataset/Tom_Cruise_dataset_two_faces/testing/real/TOM CRUISE 2020 Deepfake  VFX Breakdown  Side by Side_1080p_Trim\"\n",
        "\n",
        "batch_input1=get_input(test_folder1)\n",
        "batchx1=np.array(batch_input1)\n",
        "\n",
        "batch_input2=get_input(test_folder2)\n",
        "batchx2=np.array(batch_input2)\n",
        "\n",
        "model=load_model(\"All_Models/Tom_Cruise/tom_cruise_xceptionimagenet_attempt1/_xceptionimagenet_2054_128_finetuned.h5\")\n",
        "result1=model.predict(batchx2)\n",
        "print(result1)\n",
        "\n",
        "result2=model.predict(batchx1)\n",
        "print(result2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkif1y84UNxi"
      },
      "source": [
        "CNN + LSTM Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia3pEwIoUNxk"
      },
      "source": [
        "\n",
        "video_folder=\"../Tom_Cruise/Completeframe/TOM_CRUISE_2020_Deepfake_VFX_Breakdown_Side_by_Side_1080p_Trim\"\n",
        "\n",
        "path_names=video_folder.split('/')\n",
        "prediction_folder=\"Predictions/\"+path_names[-1]+\"/predicted_frames\"\n",
        "ssh = Path(prediction_folder)\n",
        "ssh.mkdir(parents=True)\n",
        "\n",
        "frames=os.listdir(video_folder+\"/\")\n",
        "frames.sort()\n",
        "\n",
        "if result1[0][0]>=0.5:\n",
        "    print(result1)\n",
        "    for i in range(0,2241):\n",
        "        if frames[i] ==\".ipynb_checkpoints\":\n",
        "            continue\n",
        "        image=cv2.imread(video_folder+\"/\"+frames[i])\n",
        "        if i<=216:\n",
        "            destination=cv2.rectangle(image, (3,100), ((image.shape[1]//2)-20,image.shape[0]-100),(0,0,255), 5) \n",
        "        elif i<=290:\n",
        "            destination=cv2.rectangle(image, (3,100), ((image.shape[1]//2)-20-50,image.shape[0]-100),(0,0,255), 5) \n",
        "        else:\n",
        "            destination=cv2.rectangle(image, (3,100), ((image.shape[1]//2)-20+30,image.shape[0]-100),(0,0,255), 5) \n",
        "        #wide,length,_=(destination.shape)\n",
        "        cv2.putText(destination, 'Fake:'+str(round(result1[0][0],4)), (30,image.shape[0]-100-35), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0,0,255),thickness=2, lineType=cv2.LINE_AA)\n",
        "        cv2.imwrite(prediction_folder+\"/\"+frames[i],destination)\n",
        "else:\n",
        "    print(result1)\n",
        "    for i in range(0,2241):\n",
        "        if frames[i] ==\".ipynb_checkpoints\":\n",
        "            continue\n",
        "        image=cv2.imread(video_folder+\"/\"+frames[i])\n",
        "        if i<=216:\n",
        "            destination=cv2.rectangle(image, (3,100), ((image.shape[1]//2)-20,image.shape[0]-100),(0,255,0), 5) \n",
        "        elif i<=290:\n",
        "            destination=cv2.rectangle(image, (3,100), ((image.shape[1]//2)-20-50,image.shape[0]-100),(0,255,0), 5) \n",
        "        else:\n",
        "            destination=cv2.rectangle(image, (3,100), ((image.shape[1]//2)-20+30,image.shape[0]-100),(0,255,0), 5) \n",
        "        #wide,length,_=(destination.shape)\n",
        "        cv2.putText(destination, 'Real:'+str(round(result1[0][0],4)), (30,image.shape[0]-100-35), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (35,102,11),thickness=2, lineType=cv2.LINE_AA)\n",
        "        cv2.imwrite(prediction_folder+\"/\"+frames[i],destination)\n",
        "if result2[0][0]>=0.5:\n",
        "    print(result1)\n",
        "    for i in range(0,2241):\n",
        "        if frames[i] ==\".ipynb_checkpoints\":\n",
        "            continue\n",
        "        image=cv2.imread(prediction_folder+\"/\"+frames[i])\n",
        "        if i<=216:\n",
        "            destination=cv2.rectangle(image,  ((image.shape[1]//2)-15,100),((image.shape[1])-3,image.shape[0]-100),(0,0,255), 5) \n",
        "        elif i<=290:\n",
        "            destination=cv2.rectangle(image,  ((image.shape[1]//2)-20-45,100),((image.shape[1])-3,image.shape[0]-100),(0,0,255), 5) \n",
        "        else:\n",
        "            destination=cv2.rectangle(image, ((image.shape[1]//2)-20+35,100),((image.shape[1])-3,image.shape[0]-100),(0,0,255), 5) \n",
        "        wide,length,_=(destination.shape)\n",
        "        cv2.putText(destination, 'Fake:'+str(round(result2[0][0],4)), ((image.shape[1])-330,wide-100-35), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0,0,255),thickness=2, lineType=cv2.LINE_AA)\n",
        "        cv2.imwrite(prediction_folder+\"/\"+frames[i],destination)\n",
        "else:\n",
        "    print(result2)\n",
        "    print(result1)\n",
        "    for i in range(0,2241):\n",
        "        if frames[i] ==\".ipynb_checkpoints\":\n",
        "            continue\n",
        "        image=cv2.imread(prediction_folder+\"/\"+frames[i])\n",
        "        if i<=216:\n",
        "            destination=cv2.rectangle(image,((image.shape[1]//2)-15,100),((image.shape[1])-3,image.shape[0]-100),(0,255,0), 5) \n",
        "        elif i<=290:\n",
        "            destination=cv2.rectangle(image,((image.shape[1]//2)-20-45,100),((image.shape[1])-3,image.shape[0]-100),(0,255,0), 5) \n",
        "        else:\n",
        "            destination=cv2.rectangle(image,((image.shape[1]//2)-20+35,100),((image.shape[1])-3,image.shape[0]-100),(0,255,0), 5) \n",
        "        wide,length,_=(destination.shape)\n",
        "        cv2.putText(destination, 'Real:'+str(round(result2[0][0],4)), ((image.shape[1])-330,wide-100-35), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (0,255,0),thickness=2, lineType=cv2.LINE_AA)\n",
        "        cv2.imwrite(prediction_folder+\"/\"+frames[i],destination)\n",
        "\n",
        "        \n",
        "images=[]\n",
        "files=os.listdir(prediction_folder)\n",
        "files.sort()\n",
        "print(files)\n",
        "\n",
        "for filename in files:\n",
        "    try:    \n",
        "        img = cv2.imread(prediction_folder+\"/\"+filename)\n",
        "        height, width, layers = img.shape\n",
        "        height, width, layers = img.shape\n",
        "        size = (width,height)\n",
        "        images.append(img)\n",
        "    except:\n",
        "        continue\n",
        "        \n",
        "print(\"NOW RECONSTRUCTING---\")\n",
        "    \n",
        "out = cv2.VideoWriter(\"Predictions/\"+path_names[-1]+'/predicted_video30.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 30, size)\n",
        "\n",
        "for i in range(len(images)):\n",
        "    out.write(images[i])\n",
        "\n",
        "out.release()\n",
        "\n",
        "print(\"VIDEO COMPLETED!!!\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTaKoB6PUNxp"
      },
      "source": [
        "## Video Deepfake Detection Using GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxrc-FB0UNxr"
      },
      "source": [
        "epochs = 40\n",
        "batch_size = 16\n",
        "\n",
        "train_spe = len(os.listdir(dataset+\"/\"+'real'))//batch_size\n",
        "print(train_spe)\n",
        "val_spe = len(os.listdir(dataset+\"/\"+'real'))//batch_size\n",
        "print(val_spe)\n",
        "test_spe = len(os.listdir(dataset+\"/\"+'real'))//(batch_size)\n",
        "print(test_spe)\n",
        "\n",
        "print('For GRU:')\n",
        "train_datagen = video_data_generator(dataset,'train', batch_size = batch_size)\n",
        "val_datagen = video_data_generator(dataset,'val', batch_size = batch_size)\n",
        "test_datagen = video_data_generator(dataset,'test', batch_size = batch_size)\n",
        "print(\"For model without fine-tuning validation loss and validation accuracy is:\")\n",
        "cnngru_model = load_model('models/TDCNN_GRU_20_Epochs/multiframe_full_xceptionimagenet_attempt2/multiframe_full_xceptionimagenet_10_128.h5')\n",
        "test_datagen = video_data_generator(dataset,'test', batch_size = batch_size)\n",
        "start_time1 = time.clock()\n",
        "print(cnngru_model.evaluate(test_datagen,steps=2*test_spe,verbose=0))\n",
        "current_time1= time.clock()\n",
        "print(\"Evaluation time required without fine tuning:\")\n",
        "print(current_time1-start_time1)\n",
        "\n",
        "yhat_classes=cnngru_model.predict(test_datagen,steps=2*test_spe,verbose=0)\n",
        "accuracy = accuracy_score(testy, yhat_classes.round())\n",
        "print('Accuracy:   ',accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(testy, yhat_classes.round())\n",
        "print(\"Precision:  \",precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(testy, yhat_classes.round())\n",
        "print('Recall:     ',recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(testy, yhat_classes.round())\n",
        "print('f1 score:   ',f1)\n",
        "\n",
        "\n",
        "print(\"For model with fine-tuning validation loss and validation accuracy is:\")\n",
        "cnngru_model = load_model('models/TDCNN_GRU_20_Epochs/multiframe_full_xceptionimagenet_attempt2/multiframe_full_xceptionimagenet_10_128_finetuned.h5')\n",
        "test_datagen = video_data_generator(dataset,'test', batch_size = batch_size)\n",
        "start_time_1 = time.clock()\n",
        "print(cnngru_model.evaluate(test_datagen,steps=2*test_spe,verbose=0))\n",
        "current_time_1 = time.clock()\n",
        "print(\"Evaluation time required with fine tuning:\")\n",
        "print(current_time_1-start_time_1)\n",
        "\n",
        "yhat_classes=cnngru_model.predict(test_datagen,steps=2*test_spe,verbose=0)\n",
        "accuracy = accuracy_score(testy, yhat_classes.round())\n",
        "print('Accuracy:   ',accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(testy, yhat_classes.round())\n",
        "print(\"Precision:  \",precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(testy, yhat_classes.round())\n",
        "print('Recall:     ',recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(testy, yhat_classes.round())\n",
        "print('f1 score:   ',f1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}